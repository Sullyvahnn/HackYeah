import scrapy
import sys
import os
import time

# Dodaj Å›cieÅ¼kÄ™ do src/
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from agent.crime_news_scrapper.ai_filter import CrimeFilter 

class CrimeNewsSpider(scrapy.Spider):
    """Spider do scrapowania artykuÅ‚Ã³w o przestÄ™pstwach"""
    name = 'crime_news'
    
    start_urls = [
        'https://tvn24.pl/krakow',
    ]
    
    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'ROBOTSTXT_OBEY': True,
        'CONCURRENT_REQUESTS': 4,
        'DOWNLOAD_DELAY': 2,
        'COOKIES_ENABLED': False,
        'LOG_LEVEL': 'INFO'
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        init_start_time = time.time()
        self.logger.info("ğŸ¤– InicjalizujÄ™ AI Filter...")
        
        self.ai_filter = CrimeFilter() 
        
        init_elapsed = time.time() - init_start_time
        self.logger.info(f"âœ… AI Filter zainicjalizowany w: {init_elapsed:.2f}s")
        
        self.scraped_count = 0
        self.filtered_count = 0
        self.passed_count = 0
    
    def parse(self, response):
        """GÅ‚Ã³wna metoda parsowania i wstÄ™pnej filtracji"""
        self.logger.info(f"ğŸ” ScrapujÄ™: {response.url}")
        
        # Selektory dla TVN24 - artykuÅ‚y z linkami
        for article in response.css('article a[href], div.article-item a[href]'):
            url = article.css('::attr(href)').get()
            title = article.css('::attr(title)').get() or article.css('::text').get()
            
            # Zajawka moÅ¼e byÄ‡ w rÃ³Å¼nych miejscach
            teaser = article.css('p::text, span.description::text').get()

            if title and url:
                # Upewnij siÄ™, Å¼e URL jest peÅ‚ny
                full_url = response.urljoin(url)
                source = response.url.split('/')[2]
                
                self.scraped_count += 1
                
                # Krok 1: WstÄ™pna Filtracja AI
                if self.ai_filter.is_crime_related(title, teaser or ''):
                    self.passed_count += 1
                    self.logger.info(f"âœ… [{self.passed_count}] PrzeszÅ‚o filtr: {title[:60]}...")
                    
                    yield scrapy.Request(
                        full_url,
                        callback=self.parse_full_article,
                        meta={'title': title, 'source': source, 'url': full_url},
                        errback=self.handle_error,
                        dont_filter=False  # PozwÃ³l Scrapy filtrowaÄ‡ duplikaty URL
                    )
                else:
                    self.filtered_count += 1
                    self.logger.debug(f"âŒ [{self.filtered_count}] Odrzucono: {title[:60]}...")
        
        # PodÄ…Å¼aj za nastÄ™pnymi stronami paginacji (opcjonalnie)
        next_page = response.css('a.next::attr(href), a[rel="next"]::attr(href)').get()
        if next_page:
            self.logger.info(f"â¡ï¸ PrzechodzÄ™ do nastÄ™pnej strony")
            yield response.follow(next_page, self.parse)
        
    def parse_full_article(self, response):
        """Pobiera peÅ‚ny tekst artykuÅ‚u i przekazuje do Pipeline"""
        title = response.meta['title']
        source = response.meta['source']
        url = response.meta['url']
        
        # Poprawiony selektor dla treÅ›ci artykuÅ‚u TVN24
        paragraphs = response.css('''
            article p::text,
            div.article-body p::text,
            div.article__body p::text,
            div[class*="content"] p::text,
            div.text-content p::text
        ''').getall()
        
        raw_text = '\n'.join(p.strip() for p in paragraphs if p.strip())
        
        if raw_text and len(raw_text) > 100:
            self.logger.info(f"ğŸ“„ PeÅ‚ny tekst pobrany ({len(raw_text)} znakÃ³w): {title[:50]}...")
            # Krok 2: Zwracanie danych do Pipeline
            yield { 
                'url': url,
                'title': title,
                'raw_text': raw_text,
                'source': source
            }
        else:
            self.logger.warning(f"âš ï¸ Za maÅ‚o tekstu ({len(raw_text)} znakÃ³w): {url}")
    
    def handle_error(self, failure):
        self.logger.error(f"âŒ BÅ‚Ä…d pobierania: {failure.request.url}")
    
    def closed(self, reason):
        self.logger.info("\n" + "="*70)
        self.logger.info(f"ğŸ“Š PODSUMOWANIE SCRAPOWANIA")
        self.logger.info("="*70)
        self.logger.info(f"  ğŸ” Znalezione artykuÅ‚y: {self.scraped_count}")
        self.logger.info(f"  âœ… PrzeszÅ‚o filtr AI: {self.passed_count}")
        self.logger.info(f"  âŒ Odrzucone przez AI: {self.filtered_count}")
        if self.scraped_count > 0:
            self.logger.info(f"  ğŸ“ˆ WskaÅºnik filtracji: {(self.passed_count/self.scraped_count*100):.1f}%")
        self.logger.info("="*70 + "\n")